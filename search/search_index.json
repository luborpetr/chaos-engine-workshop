{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"In this workshop you will learn how to deploy Chaos Engine framework and how to execute and analyse experiments targeting Kubernetes PODs. The Chaos Engine and our test targets will be provisioned in Google Cloud Platform. Workshop Prerequisites Sign up for free GCP subscription Make sure you have Chrome web browser installed","title":"Home"},{"location":"#workshop-prerequisites","text":"Sign up for free GCP subscription Make sure you have Chrome web browser installed","title":"Workshop Prerequisites"},{"location":"Labs/Lab 1/","text":"Lab 1: Activate your GCP console Open Chrome web browser and go to GCP console Activate Cloud Shell by clinking on the shell icon in the upper right corner of the Console I will take a while until your Cloud Shell instance gets provisioned","title":"Lab 1: Activate your GCP console"},{"location":"Labs/Lab 1/#lab-1-activate-your-gcp-console","text":"Open Chrome web browser and go to GCP console Activate Cloud Shell by clinking on the shell icon in the upper right corner of the Console I will take a while until your Cloud Shell instance gets provisioned","title":"Lab 1: Activate your GCP console"},{"location":"Labs/Lab 2/","text":"Lab 2: Install Chaos Engine In this lab we will setup our demo environment and we will learn how to deploy and run Chaos Engine tool. Clone workshop GitHub repo In Cloud Shell clone the workshop repo. We are going to use scripts located in scripts directory. shell command git clone https://github.com/luborpetr/chaos-engine-workshop.git expected output Cloning into 'chaos-engine-workshop'... remote: Enumerating objects: 4, done. remote: Counting objects: 100% (4/4), done. remote: Compressing objects: 100% (3/3), done. remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (4/4), done. Create new compute instance We start with creation of a compute node that will be hosting our Chaos Engine instance. Run following gcloud command in your Cloud Shell . The script will provision new compute instance with docker and docker-compose installed. Note The script referenced by startup-script=scripts/provision-vm.sh option is located in the repo we've downloaded in previous step. Make sure you are in the repo root directory before you run the gcloud command. gcloud command gcloud compute \\ instances create chaos-engine \\ --zone = europe-west2-c \\ --machine-type = n1-standard-1 \\ --no-service-account \\ --no-scopes \\ --tags = chaos-engine \\ --image = ubuntu-1604-xenial-v20200129 \\ --image-project = ubuntu-os-cloud \\ --boot-disk-size = 20GB \\ --boot-disk-type = pd-standard \\ --boot-disk-device-name = chaos-engine \\ --metadata-from-file startup-script = scripts/provision-vm.sh expected output WARNING : You have selected a disk size of under [ 200 GB ]. This may result in poor I /O performance. For more information, see: https://developers.google.com/compute/docs/ disks # performance . Created [ https :// www . googleapis . com /compute/v1/projects/xxxx/zones/europe-west2-c/instances/ chaos - engine ]. WARNING : Some requests generated warnings : - Disk size : '20 GB' is larger than image size : '10 GB' . You might need to resize the root repartition manually if the operating system does not support automatic resizing . See https :// cloud . google . com /compute/docs/disks/ add - persistent - disk # resize_pd for details . NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS chaos - engine europe - west2 - c n1 - standard - 1 10 . xxx . xxx . xxx xxx . xxx . xxx . xxx RUNNING Configure firewall By default all ingress traffic is dropped by the firewall. We need couple of ports opened to the internet. That could be done by following command: gcloud command gcloud compute firewall-rules create chaos-engine-inbound \\ --direction = INGRESS \\ --priority = 1000 \\ --network = default \\ --action = ALLOW \\ --rules = tcp:8080,tcp:8200,tcp:9000,tcp:5222,tcp:5280,tcp:5269 \\ --source-ranges = 0 .0.0.0/0 \\ --target-tags = chaos-engine expected command Creating firewall...\u2827Created [https://www.googleapis.com/compute/v1/projects/xxx/global/firewalls/chaos-engine-inbound]. Creating firewall...done. NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED chaos-engine-inbound default INGRESS 1000 tcp:8080,tcp:8200,tcp:9000,tcp:5222,tcp:5280,tcp:5269 False Connect to the Chaos Engine instance In order to SSH to the machine run following command gcloud command gcloud compute ssh --zone \"europe-west2-c\" \"chaos-engine\" expected output Welcome to Ubuntu 16.04.6 LTS ( GNU / Linux 4.15.0 - 1052 - gcp x86_64 ) * Documentation : https : // help . ubuntu . com * Management : https : // landscape . canonical . com * Support : https : // ubuntu . com / advantage 25 packages can be updated . 15 updates are security updates . New release '18.04.4 LTS' available . Run 'do-release-upgrade' to upgrade to it . Last login : Fri Feb 21 14 : 21 : 29 2020 from xxx . xxx . xxx . xx user @chaos - engine : ~ $ Verify all prerequisites are installed properly On the chaos-engine machine run following list of post install checks Check docker Check you have docker configured properly. shell command docker ps expected output user@chaos-engine:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES If the output looks like below, your user is not in docker group. Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied Adjust your groups and relogin sudo usermod -a -G docker $USER Check docker-compose Verify docker-compose in your path. shell command docker-compose expected output user@chaos-engine:~$ docker-compose -v docker-compose version 1.25.4, build 8d51620a Deploy Chaos Engine Chaos Engine deployment is very easy, we just need to do few configuration steps. Clone Chaos Engine GitHub repo Clone official Chaos Engine GitHub repo in order to get latest version of the configuration scripts. From your Chaos Engine VM run: shell command git clone https://github.com/ThalesGroup/chaos-engine.git expected output user@chaos-engine:~$ git clone https://github.com/ThalesGroup/chaos-engine.git Cloning into 'chaos-engine'... remote: Enumerating objects: 691, done. remote: Counting objects: 100% (691/691), done. remote: Compressing objects: 100% (315/315), done. remote: Total 26251 (delta 275), reused 609 (delta 228), pack-reused 25560 Receiving objects: 100% (26251/26251), 3.53 MiB | 3.98 MiB/s, done. Resolving deltas: 100% (10854/10854), done. Checking connectivity... done. Adjust configuration On the Chaos Engine machine go to chaos-engine directory and replace docker-compose.yml with a file from the workshop repo: wget -O docker-compose.yml https://raw.githubusercontent.com/luborpetr/chaos-engine-workshop/master/docker/docker-compose.yml Pull Docker Images Pull Chaos Engine image from DockerHub. shell command docker-compose pull expected output Pulling vault ... done Pulling vault-loader ... done Pulling chaosengine ... done Verify you pulled image tagged stable . shell command docker images expected output REPOSITORY TAG IMAGE ID CREATED SIZE thalesgroup/chaos-engine stable 46c560a17d9a 2 days ago 304MB vault latest 0542f65ae3d0 4 weeks ago 140MB Verify Chaos Engine deployment Start Chaos Engine framework using docker-compose . shell command docker-compose up expected output { \"@timestamp\" : \"2020-02-24T08:37:45.313Z\" , \"@version\" : \"1\" , \"message\" : \"There are no platforms enabled\" , \"logger_name\" : \"com.thales.chaos.experiment.ExperimentManager\" , \"thread_name\" : \"chaos-1\" , \"level\" : \"WARN\" , \"level_value\" : 30000 , \"env\" : \"WORKSHOP\" , \"chaos-host\" : \"904cbd65faa1@gcp:chaos-engine:projects/203123834228/zones/europe-west2-c\" } Verify that Chaos Engine endpoints are listening. From you local machine visit following URLS: First URL is a Chaos Engine API endpoint. After we complete Engine configuration the OpenAPI UI will be exposed there. But know expected output is and 404 . shell command http:// ${ CHAOS_ENGINE_IP } :8080 expected output Whitelabel Error Page This application has no explicit mapping for /error, so you are seeing this as a fallback. Second URL is Vault UI. You should be able to sing in using token 00000000-0000-0000-0000-000000000000 . shell command http:// ${ CHAOS_ENGINE_IP } :8200 expected output Vault sign in page Lab summary At the end of this exercise you should have: Basic understanding of the GCP console features and layout Chaos Engine injector machine up and running Chaos Engine framework deployed","title":"Lab 2: Install Chaos Engine"},{"location":"Labs/Lab 2/#lab-2-install-chaos-engine","text":"In this lab we will setup our demo environment and we will learn how to deploy and run Chaos Engine tool.","title":"Lab 2: Install Chaos Engine"},{"location":"Labs/Lab 2/#clone-workshop-github-repo","text":"In Cloud Shell clone the workshop repo. We are going to use scripts located in scripts directory. shell command git clone https://github.com/luborpetr/chaos-engine-workshop.git expected output Cloning into 'chaos-engine-workshop'... remote: Enumerating objects: 4, done. remote: Counting objects: 100% (4/4), done. remote: Compressing objects: 100% (3/3), done. remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (4/4), done.","title":"Clone workshop GitHub repo"},{"location":"Labs/Lab 2/#create-new-compute-instance","text":"We start with creation of a compute node that will be hosting our Chaos Engine instance. Run following gcloud command in your Cloud Shell . The script will provision new compute instance with docker and docker-compose installed. Note The script referenced by startup-script=scripts/provision-vm.sh option is located in the repo we've downloaded in previous step. Make sure you are in the repo root directory before you run the gcloud command. gcloud command gcloud compute \\ instances create chaos-engine \\ --zone = europe-west2-c \\ --machine-type = n1-standard-1 \\ --no-service-account \\ --no-scopes \\ --tags = chaos-engine \\ --image = ubuntu-1604-xenial-v20200129 \\ --image-project = ubuntu-os-cloud \\ --boot-disk-size = 20GB \\ --boot-disk-type = pd-standard \\ --boot-disk-device-name = chaos-engine \\ --metadata-from-file startup-script = scripts/provision-vm.sh expected output WARNING : You have selected a disk size of under [ 200 GB ]. This may result in poor I /O performance. For more information, see: https://developers.google.com/compute/docs/ disks # performance . Created [ https :// www . googleapis . com /compute/v1/projects/xxxx/zones/europe-west2-c/instances/ chaos - engine ]. WARNING : Some requests generated warnings : - Disk size : '20 GB' is larger than image size : '10 GB' . You might need to resize the root repartition manually if the operating system does not support automatic resizing . See https :// cloud . google . com /compute/docs/disks/ add - persistent - disk # resize_pd for details . NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS chaos - engine europe - west2 - c n1 - standard - 1 10 . xxx . xxx . xxx xxx . xxx . xxx . xxx RUNNING","title":"Create new compute instance"},{"location":"Labs/Lab 2/#configure-firewall","text":"By default all ingress traffic is dropped by the firewall. We need couple of ports opened to the internet. That could be done by following command: gcloud command gcloud compute firewall-rules create chaos-engine-inbound \\ --direction = INGRESS \\ --priority = 1000 \\ --network = default \\ --action = ALLOW \\ --rules = tcp:8080,tcp:8200,tcp:9000,tcp:5222,tcp:5280,tcp:5269 \\ --source-ranges = 0 .0.0.0/0 \\ --target-tags = chaos-engine expected command Creating firewall...\u2827Created [https://www.googleapis.com/compute/v1/projects/xxx/global/firewalls/chaos-engine-inbound]. Creating firewall...done. NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED chaos-engine-inbound default INGRESS 1000 tcp:8080,tcp:8200,tcp:9000,tcp:5222,tcp:5280,tcp:5269 False","title":"Configure firewall"},{"location":"Labs/Lab 2/#connect-to-the-chaos-engine-instance","text":"In order to SSH to the machine run following command gcloud command gcloud compute ssh --zone \"europe-west2-c\" \"chaos-engine\" expected output Welcome to Ubuntu 16.04.6 LTS ( GNU / Linux 4.15.0 - 1052 - gcp x86_64 ) * Documentation : https : // help . ubuntu . com * Management : https : // landscape . canonical . com * Support : https : // ubuntu . com / advantage 25 packages can be updated . 15 updates are security updates . New release '18.04.4 LTS' available . Run 'do-release-upgrade' to upgrade to it . Last login : Fri Feb 21 14 : 21 : 29 2020 from xxx . xxx . xxx . xx user @chaos - engine : ~ $","title":"Connect to the Chaos Engine instance"},{"location":"Labs/Lab 2/#verify-all-prerequisites-are-installed-properly","text":"On the chaos-engine machine run following list of post install checks","title":"Verify all prerequisites are installed properly"},{"location":"Labs/Lab 2/#check-docker","text":"Check you have docker configured properly. shell command docker ps expected output user@chaos-engine:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES If the output looks like below, your user is not in docker group. Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied Adjust your groups and relogin sudo usermod -a -G docker $USER","title":"Check docker"},{"location":"Labs/Lab 2/#check-docker-compose","text":"Verify docker-compose in your path. shell command docker-compose expected output user@chaos-engine:~$ docker-compose -v docker-compose version 1.25.4, build 8d51620a","title":"Check docker-compose"},{"location":"Labs/Lab 2/#deploy-chaos-engine","text":"Chaos Engine deployment is very easy, we just need to do few configuration steps.","title":"Deploy Chaos Engine"},{"location":"Labs/Lab 2/#clone-chaos-engine-github-repo","text":"Clone official Chaos Engine GitHub repo in order to get latest version of the configuration scripts. From your Chaos Engine VM run: shell command git clone https://github.com/ThalesGroup/chaos-engine.git expected output user@chaos-engine:~$ git clone https://github.com/ThalesGroup/chaos-engine.git Cloning into 'chaos-engine'... remote: Enumerating objects: 691, done. remote: Counting objects: 100% (691/691), done. remote: Compressing objects: 100% (315/315), done. remote: Total 26251 (delta 275), reused 609 (delta 228), pack-reused 25560 Receiving objects: 100% (26251/26251), 3.53 MiB | 3.98 MiB/s, done. Resolving deltas: 100% (10854/10854), done. Checking connectivity... done.","title":"Clone Chaos Engine GitHub repo"},{"location":"Labs/Lab 2/#adjust-configuration","text":"On the Chaos Engine machine go to chaos-engine directory and replace docker-compose.yml with a file from the workshop repo: wget -O docker-compose.yml https://raw.githubusercontent.com/luborpetr/chaos-engine-workshop/master/docker/docker-compose.yml","title":"Adjust configuration"},{"location":"Labs/Lab 2/#pull-docker-images","text":"Pull Chaos Engine image from DockerHub. shell command docker-compose pull expected output Pulling vault ... done Pulling vault-loader ... done Pulling chaosengine ... done Verify you pulled image tagged stable . shell command docker images expected output REPOSITORY TAG IMAGE ID CREATED SIZE thalesgroup/chaos-engine stable 46c560a17d9a 2 days ago 304MB vault latest 0542f65ae3d0 4 weeks ago 140MB","title":"Pull Docker Images"},{"location":"Labs/Lab 2/#verify-chaos-engine-deployment","text":"Start Chaos Engine framework using docker-compose . shell command docker-compose up expected output { \"@timestamp\" : \"2020-02-24T08:37:45.313Z\" , \"@version\" : \"1\" , \"message\" : \"There are no platforms enabled\" , \"logger_name\" : \"com.thales.chaos.experiment.ExperimentManager\" , \"thread_name\" : \"chaos-1\" , \"level\" : \"WARN\" , \"level_value\" : 30000 , \"env\" : \"WORKSHOP\" , \"chaos-host\" : \"904cbd65faa1@gcp:chaos-engine:projects/203123834228/zones/europe-west2-c\" } Verify that Chaos Engine endpoints are listening. From you local machine visit following URLS: First URL is a Chaos Engine API endpoint. After we complete Engine configuration the OpenAPI UI will be exposed there. But know expected output is and 404 . shell command http:// ${ CHAOS_ENGINE_IP } :8080 expected output Whitelabel Error Page This application has no explicit mapping for /error, so you are seeing this as a fallback. Second URL is Vault UI. You should be able to sing in using token 00000000-0000-0000-0000-000000000000 . shell command http:// ${ CHAOS_ENGINE_IP } :8200 expected output Vault sign in page","title":"Verify Chaos Engine deployment"},{"location":"Labs/Lab 2/#lab-summary","text":"At the end of this exercise you should have: Basic understanding of the GCP console features and layout Chaos Engine injector machine up and running Chaos Engine framework deployed","title":"Lab summary"},{"location":"Labs/Lab 3/","text":"Lab 3: Deploy Chaos Engine Victim In this lab we are going to deploy test Kubernetes cluster in GCP with 3 worker nodes and 2 dummy applications. The cluster will be a target for our experiments we will run in the next lab. Create new Kubernetes cluster Let's provision tiny cluster with 3 nodes. Each node will have 1 CPU and 1.5 GB memory. From Cloud Shell run: gcloud command gcloud container clusters create \"chaos-engine-victim\" \\ --zone \"europe-west1-b\" \\ --no-enable-basic-auth \\ --cluster-version \"1.14.10-gke.17\" \\ --machine-type \"g1-small\" \\ --image-type \"COS\" \\ --disk-type \"pd-standard\" \\ --disk-size \"20\" \\ --num-nodes \"3\" \\ --enable-stackdriver-kubernetes \\ --enable-ip-alias \\ --enable-autoupgrade \\ --enable-autorepair expected output WARNING : Starting in 1.12 , default node pools in new clusters will have their legacy Compute Engine instance metadata endpoints disabled by default . To create a cluster with legacy instance metadata endpoints disabled in the default node pool , run ` clusters create ` with the flag ` -- metadata disable - legacy - endpoints = true ` . WARNING : The Pod address range limits the maximum size of the cluster . Please refer to https :// cloud . google . com /kubernetes-engine/docs/how-to/ flexible - pod - cidr to learn how to optimize IP address allocation . This will enable the autorepair feature for nodes . Please see https :// cloud . google . com /kubernetes-engine/docs/ node - auto - repair for more information on node autorepairs . Creating cluster chaos - engine - victim in europe - west1 - b ... Cluster is being health - checked ( master is healthy )... done . Created [ https :// container . googleapis . com /v1/projects/xxxx/zones/europe-west1-b/clusters/ chaos - engine - victim ]. To inspect the contents of your cluster , go to : https :// console . cloud . google . com /kubernetes/workload_/gcloud/europe-west1-b/ chaos - engine - victim ? project = xxxx kubeconfig entry generated for chaos - engine - victim . NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS chaos - engine - victim europe - west1 - b 1.14 . 10 - gke . 17 xxx . xxx . xxx . xxx g1 - small 1.14 . 10 - gke . 17 3 RUNNING Authenticate In order to run kubectl commands we need to add new context into your .kube config. It could be done by following gcloud command. gcloud command gcloud container clusters get-credentials chaos-engine-victim --zone europe-west1-b expected output Fetching cluster endpoint and auth data. kubeconfig entry generated for chaos-engine-victim. Check that the kubectl context has been switched. shell command kubectl config current-context expected output xxxxxxxxx-west1-b_chaos-engine-victim Deploy dummy applications In order to demonstrate Chaos Engine features we need to deploy couple of applications. We will use 2 deployments, one nginx and second apache. Please go to the workshop repo you cloned to your Cloud Shell instance and perform following command. shell command kubectl apply -f kubernetes/applications.yml expected output deployment.apps/nginx configured deployment.apps/apache created Verify test application were started. shell command kubectl get pods expected output NAME READY STATUS RESTARTS AGE apache-7c99b8d54f-bkk2w 1/1 Running 0 77m apache-7c99b8d54f-g8k6g 1/1 Running 0 77m apache-7c99b8d54f-ncwcn 1/1 Running 0 77m nginx-8779fd9dc-22hqf 1/1 Running 0 76m nginx-8779fd9dc-pt66s 1/1 Running 0 77m nginx-8779fd9dc-zn94b 1/1 Running 0 76m Configure RBAC In the last step we need to create a service account that will be used by the Engine and do a role bindings. All can be do by simply applying a template from the workshop repo. shell command kubectl apply -f kubernetes/rbac.yml expected output role.rbac.authorization.k8s.io/chaos-engine-role created serviceaccount/chaos-engine-serviceaccount created rolebinding.rbac.authorization.k8s.io/chaos-engine-rolebinding created Lab summary At the end of this exercise you should have: Running Kubernetes cluster Deployed dummy applications RBAC configured","title":"Lab 3: Deploy Chaos Engine Victim"},{"location":"Labs/Lab 3/#lab-3-deploy-chaos-engine-victim","text":"In this lab we are going to deploy test Kubernetes cluster in GCP with 3 worker nodes and 2 dummy applications. The cluster will be a target for our experiments we will run in the next lab.","title":"Lab 3: Deploy Chaos Engine Victim"},{"location":"Labs/Lab 3/#create-new-kubernetes-cluster","text":"Let's provision tiny cluster with 3 nodes. Each node will have 1 CPU and 1.5 GB memory. From Cloud Shell run: gcloud command gcloud container clusters create \"chaos-engine-victim\" \\ --zone \"europe-west1-b\" \\ --no-enable-basic-auth \\ --cluster-version \"1.14.10-gke.17\" \\ --machine-type \"g1-small\" \\ --image-type \"COS\" \\ --disk-type \"pd-standard\" \\ --disk-size \"20\" \\ --num-nodes \"3\" \\ --enable-stackdriver-kubernetes \\ --enable-ip-alias \\ --enable-autoupgrade \\ --enable-autorepair expected output WARNING : Starting in 1.12 , default node pools in new clusters will have their legacy Compute Engine instance metadata endpoints disabled by default . To create a cluster with legacy instance metadata endpoints disabled in the default node pool , run ` clusters create ` with the flag ` -- metadata disable - legacy - endpoints = true ` . WARNING : The Pod address range limits the maximum size of the cluster . Please refer to https :// cloud . google . com /kubernetes-engine/docs/how-to/ flexible - pod - cidr to learn how to optimize IP address allocation . This will enable the autorepair feature for nodes . Please see https :// cloud . google . com /kubernetes-engine/docs/ node - auto - repair for more information on node autorepairs . Creating cluster chaos - engine - victim in europe - west1 - b ... Cluster is being health - checked ( master is healthy )... done . Created [ https :// container . googleapis . com /v1/projects/xxxx/zones/europe-west1-b/clusters/ chaos - engine - victim ]. To inspect the contents of your cluster , go to : https :// console . cloud . google . com /kubernetes/workload_/gcloud/europe-west1-b/ chaos - engine - victim ? project = xxxx kubeconfig entry generated for chaos - engine - victim . NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS chaos - engine - victim europe - west1 - b 1.14 . 10 - gke . 17 xxx . xxx . xxx . xxx g1 - small 1.14 . 10 - gke . 17 3 RUNNING","title":"Create new Kubernetes cluster"},{"location":"Labs/Lab 3/#authenticate","text":"In order to run kubectl commands we need to add new context into your .kube config. It could be done by following gcloud command. gcloud command gcloud container clusters get-credentials chaos-engine-victim --zone europe-west1-b expected output Fetching cluster endpoint and auth data. kubeconfig entry generated for chaos-engine-victim. Check that the kubectl context has been switched. shell command kubectl config current-context expected output xxxxxxxxx-west1-b_chaos-engine-victim","title":"Authenticate"},{"location":"Labs/Lab 3/#deploy-dummy-applications","text":"In order to demonstrate Chaos Engine features we need to deploy couple of applications. We will use 2 deployments, one nginx and second apache. Please go to the workshop repo you cloned to your Cloud Shell instance and perform following command. shell command kubectl apply -f kubernetes/applications.yml expected output deployment.apps/nginx configured deployment.apps/apache created Verify test application were started. shell command kubectl get pods expected output NAME READY STATUS RESTARTS AGE apache-7c99b8d54f-bkk2w 1/1 Running 0 77m apache-7c99b8d54f-g8k6g 1/1 Running 0 77m apache-7c99b8d54f-ncwcn 1/1 Running 0 77m nginx-8779fd9dc-22hqf 1/1 Running 0 76m nginx-8779fd9dc-pt66s 1/1 Running 0 77m nginx-8779fd9dc-zn94b 1/1 Running 0 76m","title":"Deploy dummy applications"},{"location":"Labs/Lab 3/#configure-rbac","text":"In the last step we need to create a service account that will be used by the Engine and do a role bindings. All can be do by simply applying a template from the workshop repo. shell command kubectl apply -f kubernetes/rbac.yml expected output role.rbac.authorization.k8s.io/chaos-engine-role created serviceaccount/chaos-engine-serviceaccount created rolebinding.rbac.authorization.k8s.io/chaos-engine-rolebinding created","title":"Configure RBAC"},{"location":"Labs/Lab 3/#lab-summary","text":"At the end of this exercise you should have: Running Kubernetes cluster Deployed dummy applications RBAC configured","title":"Lab summary"},{"location":"Labs/Lab 4/","text":"Lab 4: Finalise Framework Configuration Very well, we have Chaos Engine and Kubernetes cluster deployed, last step before we can run experiments is provisioning of the Vault secure store. Retrieve a token Retrieve the token linked to your chaos-engine-serviceaccount . shell command kubectl describe secret chaos-engine expected output Name : chaos - engine - serviceaccount - token - 9 thjd Namespace : default Labels : < none > Annotations : kubernetes . io / service - account . name : chaos - engine - serviceaccount kubernetes . io / service - account . uid : 87 b58785 - 5719 - 11 ea - 90 b5 - 42010 a8400d8 Type : kubernetes . io / service - account - token Data ==== namespace : 7 bytes token : eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9 ...... ca . crt : 1115 bytes Prepare a config file We are going to load configuration into the Vault. The Vault accepts input data in JSON format. In order to activate necessary Chaos Engine modules we need to define following variables. { \"holidays\" : \"NONSTOP\" , \"automatedMode\" : \"false\" , \"chaos.security.enabled\" : \"false\" , \"kubernetes\" : \"\" , \"kubernetes.url\" : \"https://{KUBERNETES_CLUSTER_IP}\" , \"kubernetes.token\" : \"eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9....\" , \"kubernetes.averageMillisPerExperiment\" : \"30000\" } SSH to your chaos-engine VM. Go to the chaos-engine repo and create new file ./developer-tools/vault-loader/vault-secrets.json containing JSON object from previous paragraph. Provision Vault shell command docker-compose build vault-loader expected output Building vault - loader Step 1 / 6 : FROM vault : latest ---> 0542f65ae3d0 Step 2 / 6 : WORKDIR / vault - loader / ---> Using cache ---> 7b7931c22a68 Step 3 / 6 : ADD . / vault -* . / ---> 1f9bb91ebaad Step 4 / 6 : RUN touch . / vault - secrets . json ---> Running in d1ad93796cdf Removing intermediate container d1ad93796cdf ---> e34311e2b4b5 Step 5 / 6 : ENTRYPOINT [ \"/bin/sh\" , \"-c\" ] ---> Running in e0d04f6ea1ac Removing intermediate container e0d04f6ea1ac ---> d15ecf9a3da9 Step 6 / 6 : CMD [ \"./vault-init.sh\" ] ---> Running in 94ed0a132f1b Removing intermediate container 94 ed0a132f1b ---> 97224d8661fa Successfully built 97224 d8661fa Successfully tagged chaos - engine_vault - loader : latest Start the Chaos Engine Start Chaos Engine framework using docker-compose . shell command docker-compose up expected output { \"@timestamp\" : \"2020-02-24T21:17:05.723Z\" , \"@version\" : \"1\" , \"message\" : \"Kubernetes Platform created\" , \"logger_name\" : \"com.thales.chaos.platform.impl.KubernetesPlatform\" , \"thread_name\" : \"main\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"env\" : \"WORKSHOP\" , \"chaos-host\" : \"b8dcfa2ac884@gcp:chaos-engine:projects/203123834228/zones/europe-west2-c\" } Verify setup Visit Chaos Engine API endpoint at http://${CHAOS_ENGINE_IP}:8080/swagger-ui.html You should see an OpenAPI UI. Go to the Platform section and invoke Get Platforms . If the output contains 6 targets your environment is ready for experiments. shell command curl -X GET \"http:// ${ CHAOS_ENGINE_IP } :8080/platform\" expected output [ { \"roster\" : [ { \"shellCapabilities\" : {}, \"uuid\" : \"a41694a5-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"nginx-8779fd9dc-zn94b\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-zn94b (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 2084787268 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] }, { \"shellCapabilities\" : {}, \"uuid\" : \"9da33aca-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"nginx-8779fd9dc-pt66s\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-pt66s (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 2538209780 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] }, { \"shellCapabilities\" : {}, \"uuid\" : \"9dba6965-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-ncwcn\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-ncwcn (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 1977081181 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] }, { \"shellCapabilities\" : {}, \"uuid\" : \"a5ba334a-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"nginx-8779fd9dc-22hqf\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-22hqf (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 3217786026 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] }, { \"shellCapabilities\" : {}, \"uuid\" : \"9dbbddd4-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-bkk2w\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-bkk2w (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 3099393506 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] }, { \"shellCapabilities\" : {}, \"uuid\" : \"9db2b330-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-g8k6g\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-g8k6g (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 650070269 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] } ], \"experimentTimes\" : [], \"namespace\" : \"default\" , \"platformLevel\" : \"PAAS\" , \"platformHealth\" : \"OK\" , \"apiStatus\" : \"OK\" , \"nextChaosTime\" : \"2020-02-24T21:41:18.547Z\" , \"platformType\" : \"KubernetesPlatform\" , \"destructionProbability\" : 0.2 } ] Lab summary At the end of this exercise you should have: Chaos Engine configured and ready for the first round of experiments.","title":"Lab 4: Finalise Framework Configuration"},{"location":"Labs/Lab 4/#lab-4-finalise-framework-configuration","text":"Very well, we have Chaos Engine and Kubernetes cluster deployed, last step before we can run experiments is provisioning of the Vault secure store.","title":"Lab 4: Finalise Framework Configuration"},{"location":"Labs/Lab 4/#retrieve-a-token","text":"Retrieve the token linked to your chaos-engine-serviceaccount . shell command kubectl describe secret chaos-engine expected output Name : chaos - engine - serviceaccount - token - 9 thjd Namespace : default Labels : < none > Annotations : kubernetes . io / service - account . name : chaos - engine - serviceaccount kubernetes . io / service - account . uid : 87 b58785 - 5719 - 11 ea - 90 b5 - 42010 a8400d8 Type : kubernetes . io / service - account - token Data ==== namespace : 7 bytes token : eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9 ...... ca . crt : 1115 bytes","title":"Retrieve a token"},{"location":"Labs/Lab 4/#prepare-a-config-file","text":"We are going to load configuration into the Vault. The Vault accepts input data in JSON format. In order to activate necessary Chaos Engine modules we need to define following variables. { \"holidays\" : \"NONSTOP\" , \"automatedMode\" : \"false\" , \"chaos.security.enabled\" : \"false\" , \"kubernetes\" : \"\" , \"kubernetes.url\" : \"https://{KUBERNETES_CLUSTER_IP}\" , \"kubernetes.token\" : \"eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9....\" , \"kubernetes.averageMillisPerExperiment\" : \"30000\" } SSH to your chaos-engine VM. Go to the chaos-engine repo and create new file ./developer-tools/vault-loader/vault-secrets.json containing JSON object from previous paragraph.","title":"Prepare a config file"},{"location":"Labs/Lab 4/#provision-vault","text":"shell command docker-compose build vault-loader expected output Building vault - loader Step 1 / 6 : FROM vault : latest ---> 0542f65ae3d0 Step 2 / 6 : WORKDIR / vault - loader / ---> Using cache ---> 7b7931c22a68 Step 3 / 6 : ADD . / vault -* . / ---> 1f9bb91ebaad Step 4 / 6 : RUN touch . / vault - secrets . json ---> Running in d1ad93796cdf Removing intermediate container d1ad93796cdf ---> e34311e2b4b5 Step 5 / 6 : ENTRYPOINT [ \"/bin/sh\" , \"-c\" ] ---> Running in e0d04f6ea1ac Removing intermediate container e0d04f6ea1ac ---> d15ecf9a3da9 Step 6 / 6 : CMD [ \"./vault-init.sh\" ] ---> Running in 94ed0a132f1b Removing intermediate container 94 ed0a132f1b ---> 97224d8661fa Successfully built 97224 d8661fa Successfully tagged chaos - engine_vault - loader : latest","title":"Provision Vault"},{"location":"Labs/Lab 4/#start-the-chaos-engine","text":"Start Chaos Engine framework using docker-compose . shell command docker-compose up expected output { \"@timestamp\" : \"2020-02-24T21:17:05.723Z\" , \"@version\" : \"1\" , \"message\" : \"Kubernetes Platform created\" , \"logger_name\" : \"com.thales.chaos.platform.impl.KubernetesPlatform\" , \"thread_name\" : \"main\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"env\" : \"WORKSHOP\" , \"chaos-host\" : \"b8dcfa2ac884@gcp:chaos-engine:projects/203123834228/zones/europe-west2-c\" }","title":"Start the Chaos Engine"},{"location":"Labs/Lab 4/#verify-setup","text":"Visit Chaos Engine API endpoint at http://${CHAOS_ENGINE_IP}:8080/swagger-ui.html You should see an OpenAPI UI. Go to the Platform section and invoke Get Platforms . If the output contains 6 targets your environment is ready for experiments. shell command curl -X GET \"http:// ${ CHAOS_ENGINE_IP } :8080/platform\" expected output [ { \"roster\" : [ { \"shellCapabilities\" : {}, \"uuid\" : \"a41694a5-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"nginx-8779fd9dc-zn94b\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-zn94b (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 2084787268 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] }, { \"shellCapabilities\" : {}, \"uuid\" : \"9da33aca-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"nginx-8779fd9dc-pt66s\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-pt66s (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 2538209780 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] }, { \"shellCapabilities\" : {}, \"uuid\" : \"9dba6965-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-ncwcn\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-ncwcn (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 1977081181 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] }, { \"shellCapabilities\" : {}, \"uuid\" : \"a5ba334a-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"nginx-8779fd9dc-22hqf\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-22hqf (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 3217786026 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] }, { \"shellCapabilities\" : {}, \"uuid\" : \"9dbbddd4-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-bkk2w\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-bkk2w (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 3099393506 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] }, { \"shellCapabilities\" : {}, \"uuid\" : \"9db2b330-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-g8k6g\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-g8k6g (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 650070269 , \"experimentStartTime\" : null , \"knownMissingCapabilities\" : [] } ], \"experimentTimes\" : [], \"namespace\" : \"default\" , \"platformLevel\" : \"PAAS\" , \"platformHealth\" : \"OK\" , \"apiStatus\" : \"OK\" , \"nextChaosTime\" : \"2020-02-24T21:41:18.547Z\" , \"platformType\" : \"KubernetesPlatform\" , \"destructionProbability\" : 0.2 } ]","title":"Verify setup"},{"location":"Labs/Lab 4/#lab-summary","text":"At the end of this exercise you should have: Chaos Engine configured and ready for the first round of experiments.","title":"Lab summary"},{"location":"Labs/Lab 5/","text":"Lab 5: Run Experiments In this lab we will learn how to construct and run experiments. In general there are two types of experiments: User defined experiments - user can define list of targets as well as experiment methods Random experiments - the Engine randomly selects a target and experiment method User defined experiments Let's start with user defined experiments. Go to http://${CHAOS_ENGINE_IP}:8080/swagger-ui.html and find /experiment/build/ API endpoint. The Chaos Engine accepts experiment definition in JSON format. The JSON object is called experiment suite . The experiment suite definition has following format: platformType - defines on which level we are going to run our experiments. Only one platformType can be specified in the suite experimentCriteria - defines target selection criteria containerIdentifier - depending on the configuration containerIdentifier can be identifier of individual container or identifier of a logical container group. In our case it's a ReplicaSet name. experimentMethods - experiment method name specificContainerTargets - defines concrete targets of experiments Create own experiment suite In previous labs we deployed Kubernetes cluster together with dummy test applications. Now we need to gather ReplicaSet name. From the Cloud Shell run: shell command kubectl describe pod nginx | grep ReplicaSet | head -n 1 | cut -d / -f 2 Note The Chaos Engine automated mode is disabled. The Engine will perform experiments on user requests only. You need to change the scheduler mode to automated in order to let the Engine decide when and how the experiments will be executed. One target Let's define new experiment suite and target nginx deployment. With following configuration the Chaos Engine randomly selects containers from defined ReplicaSet and runs deletePod experiment. request { \"platformType\" : \"KubernetesPlatform\" , \"experimentCriteria\" :[ { \"containerIdentifier\" : \"${REPLICASET_NAME}\" , \"experimentMethods\" :[ \"deletePod\" ], \"specificContainerTargets\" :[ ] } ] } response [ { \"id\" : \"9d8ceadd-c041-4b03-8387-672c99d84279\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"ea25ec0a-5873-11ea-8e60-42010a840252\" , \"podName\" : \"nginx-8779fd9dc-r4s8g\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-r4s8g (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 460354102 , \"experimentStartTime\" : \"2020-02-26T08:49:08.337167Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-26T08:49:08.337167Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"deletePod\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null } ] Multiple targets There might be cases where you need to target multiple ReplicaSets. With following suite the Chaos Engine will perform deletePod experiment one the first and cpuBurn.sh and memoryConsumer.sh on the second. request { \"platformType\" : \"KubernetesPlatform\" , \"experimentCriteria\" :[ { \"containerIdentifier\" : \"${REPLICASET_NAME}\" , \"experimentMethods\" :[ \"deletePod\" ], \"specificContainerTargets\" :[ ] }, { \"containerIdentifier\" : \"${REPLICASET_NAME}\" , \"experimentMethods\" :[ \"cpuBurn.sh\" , \"memoryConsumer.sh\" ], \"specificContainerTargets\" :[ ] } ] } response [ { \"id\" : \"6c862bea-31b7-410e-8c89-66f058dbb9d5\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"9dba6965-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-ncwcn\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-ncwcn (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 1977081181 , \"experimentStartTime\" : \"2020-02-25T17:31:46.880036Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-25T17:31:46.880036Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"cpuBurn.sh\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null }, { \"id\" : \"2d8f0780-a792-4740-b08b-b5bd9c68082e\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"a5ba334a-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"nginx-8779fd9dc-22hqf\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-22hqf (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 3217786026 , \"experimentStartTime\" : \"2020-02-25T17:31:46.879668Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-25T17:31:46.879668Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"deletePod\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null }, { \"id\" : \"e11781f3-2a13-44aa-80b1-6d6edb3bce85\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"9dbbddd4-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-bkk2w\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-bkk2w (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 3099393506 , \"experimentStartTime\" : \"2020-02-25T17:31:46.881773Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-25T17:31:46.881773Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"memoryConsumer.sh\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null } ] Specific targets There is a way how you can select specific PODs to be experiment on. The suite below will run deletePod experiment on two selected PODs. request { \"platformType\" : \"KubernetesPlatform\" , \"experimentCriteria\" :[ { \"containerIdentifier\" : \"${REPLICASET_NAME}\" , \"experimentMethods\" :[ \"deletePod\" , \"deletePod\" ], \"specificContainerTargets\" :[ \"${CONTAINER_UUID}\" , \"${CONTAINER_UUID}\" ] } ] } response [ { \"id\" : \"2f04f8e0-2c70-418b-b207-fa75291899b3\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"b5bfbca5-57f4-11ea-b766-42010a840276\" , \"podName\" : \"nginx-8779fd9dc-8j5nz\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-8j5nz (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 2721520672 , \"experimentStartTime\" : \"2020-02-26T08:42:19.574611Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-26T08:42:19.574611Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"deletePod\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null }, { \"id\" : \"de65529a-15ca-4a75-9667-8ed28223f27d\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"f3d66972-57e9-11ea-b766-42010a840276\" , \"podName\" : \"nginx-8779fd9dc-5pm2h\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-5pm2h (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 1188653708 , \"experimentStartTime\" : \"2020-02-26T08:42:19.571086Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-26T08:42:19.571086Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"deletePod\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null } ] Random experiments You can run experiments without composing a suite. The main difference is that the Engine will randomly selects targets and experiment methods. request curl -X POST \"http://35.234.145.174:8080/experiment/start\" response [ { \"id\" : \"e4599705-0001-4170-b3bd-fd34f475b080\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"9db2b330-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-g8k6g\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-g8k6g (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 650070269 , \"experimentStartTime\" : \"2020-02-26T09:22:02.801564Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-26T09:22:02.801564Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"forkBomb.sh\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null }, { \"id\" : \"01a8d3e2-f8e4-4b7a-aeb4-0b23651a487c\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"721c1715-57f5-11ea-b766-42010a840276\" , \"podName\" : \"apache-7c99b8d54f-9txbf\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-9txbf (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 2597272431 , \"experimentStartTime\" : \"2020-02-26T09:22:02.786886Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-26T09:22:02.786886Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"starveRandomNumberGenerator.sh\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null } ] Lab summary At the end of this exercise you should know: How to construct and run experiment suite","title":"Lab 5: Run Experiments"},{"location":"Labs/Lab 5/#lab-5-run-experiments","text":"In this lab we will learn how to construct and run experiments. In general there are two types of experiments: User defined experiments - user can define list of targets as well as experiment methods Random experiments - the Engine randomly selects a target and experiment method","title":"Lab 5: Run Experiments"},{"location":"Labs/Lab 5/#user-defined-experiments","text":"Let's start with user defined experiments. Go to http://${CHAOS_ENGINE_IP}:8080/swagger-ui.html and find /experiment/build/ API endpoint. The Chaos Engine accepts experiment definition in JSON format. The JSON object is called experiment suite . The experiment suite definition has following format: platformType - defines on which level we are going to run our experiments. Only one platformType can be specified in the suite experimentCriteria - defines target selection criteria containerIdentifier - depending on the configuration containerIdentifier can be identifier of individual container or identifier of a logical container group. In our case it's a ReplicaSet name. experimentMethods - experiment method name specificContainerTargets - defines concrete targets of experiments","title":"User defined experiments"},{"location":"Labs/Lab 5/#create-own-experiment-suite","text":"In previous labs we deployed Kubernetes cluster together with dummy test applications. Now we need to gather ReplicaSet name. From the Cloud Shell run: shell command kubectl describe pod nginx | grep ReplicaSet | head -n 1 | cut -d / -f 2 Note The Chaos Engine automated mode is disabled. The Engine will perform experiments on user requests only. You need to change the scheduler mode to automated in order to let the Engine decide when and how the experiments will be executed.","title":"Create own experiment suite"},{"location":"Labs/Lab 5/#one-target","text":"Let's define new experiment suite and target nginx deployment. With following configuration the Chaos Engine randomly selects containers from defined ReplicaSet and runs deletePod experiment. request { \"platformType\" : \"KubernetesPlatform\" , \"experimentCriteria\" :[ { \"containerIdentifier\" : \"${REPLICASET_NAME}\" , \"experimentMethods\" :[ \"deletePod\" ], \"specificContainerTargets\" :[ ] } ] } response [ { \"id\" : \"9d8ceadd-c041-4b03-8387-672c99d84279\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"ea25ec0a-5873-11ea-8e60-42010a840252\" , \"podName\" : \"nginx-8779fd9dc-r4s8g\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-r4s8g (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 460354102 , \"experimentStartTime\" : \"2020-02-26T08:49:08.337167Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-26T08:49:08.337167Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"deletePod\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null } ]","title":"One target"},{"location":"Labs/Lab 5/#multiple-targets","text":"There might be cases where you need to target multiple ReplicaSets. With following suite the Chaos Engine will perform deletePod experiment one the first and cpuBurn.sh and memoryConsumer.sh on the second. request { \"platformType\" : \"KubernetesPlatform\" , \"experimentCriteria\" :[ { \"containerIdentifier\" : \"${REPLICASET_NAME}\" , \"experimentMethods\" :[ \"deletePod\" ], \"specificContainerTargets\" :[ ] }, { \"containerIdentifier\" : \"${REPLICASET_NAME}\" , \"experimentMethods\" :[ \"cpuBurn.sh\" , \"memoryConsumer.sh\" ], \"specificContainerTargets\" :[ ] } ] } response [ { \"id\" : \"6c862bea-31b7-410e-8c89-66f058dbb9d5\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"9dba6965-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-ncwcn\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-ncwcn (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 1977081181 , \"experimentStartTime\" : \"2020-02-25T17:31:46.880036Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-25T17:31:46.880036Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"cpuBurn.sh\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null }, { \"id\" : \"2d8f0780-a792-4740-b08b-b5bd9c68082e\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"a5ba334a-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"nginx-8779fd9dc-22hqf\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-22hqf (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 3217786026 , \"experimentStartTime\" : \"2020-02-25T17:31:46.879668Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-25T17:31:46.879668Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"deletePod\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null }, { \"id\" : \"e11781f3-2a13-44aa-80b1-6d6edb3bce85\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"9dbbddd4-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-bkk2w\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-bkk2w (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 3099393506 , \"experimentStartTime\" : \"2020-02-25T17:31:46.881773Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-25T17:31:46.881773Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"memoryConsumer.sh\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null } ]","title":"Multiple targets"},{"location":"Labs/Lab 5/#specific-targets","text":"There is a way how you can select specific PODs to be experiment on. The suite below will run deletePod experiment on two selected PODs. request { \"platformType\" : \"KubernetesPlatform\" , \"experimentCriteria\" :[ { \"containerIdentifier\" : \"${REPLICASET_NAME}\" , \"experimentMethods\" :[ \"deletePod\" , \"deletePod\" ], \"specificContainerTargets\" :[ \"${CONTAINER_UUID}\" , \"${CONTAINER_UUID}\" ] } ] } response [ { \"id\" : \"2f04f8e0-2c70-418b-b207-fa75291899b3\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"b5bfbca5-57f4-11ea-b766-42010a840276\" , \"podName\" : \"nginx-8779fd9dc-8j5nz\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-8j5nz (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 2721520672 , \"experimentStartTime\" : \"2020-02-26T08:42:19.574611Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-26T08:42:19.574611Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"deletePod\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null }, { \"id\" : \"de65529a-15ca-4a75-9667-8ed28223f27d\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"f3d66972-57e9-11ea-b766-42010a840276\" , \"podName\" : \"nginx-8779fd9dc-5pm2h\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"nginx-8779fd9dc\" , \"targetedSubcontainer\" : \"nginx\" , \"simpleName\" : \"nginx-8779fd9dc-5pm2h (default)\" , \"aggregationIdentifier\" : \"nginx-8779fd9dc\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 1188653708 , \"experimentStartTime\" : \"2020-02-26T08:42:19.571086Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-26T08:42:19.571086Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"deletePod\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null } ]","title":"Specific targets"},{"location":"Labs/Lab 5/#random-experiments","text":"You can run experiments without composing a suite. The main difference is that the Engine will randomly selects targets and experiment methods. request curl -X POST \"http://35.234.145.174:8080/experiment/start\" response [ { \"id\" : \"e4599705-0001-4170-b3bd-fd34f475b080\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"9db2b330-56f1-11ea-90b5-42010a8400d8\" , \"podName\" : \"apache-7c99b8d54f-g8k6g\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-g8k6g (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 650070269 , \"experimentStartTime\" : \"2020-02-26T09:22:02.801564Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-26T09:22:02.801564Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"forkBomb.sh\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null }, { \"id\" : \"01a8d3e2-f8e4-4b7a-aeb4-0b23651a487c\" , \"experimentState\" : \"CREATED\" , \"container\" : { \"shellCapabilities\" : {}, \"uuid\" : \"721c1715-57f5-11ea-b766-42010a840276\" , \"podName\" : \"apache-7c99b8d54f-9txbf\" , \"namespace\" : \"default\" , \"ownerKind\" : \"REPLICA_SET\" , \"ownerName\" : \"apache-7c99b8d54f\" , \"targetedSubcontainer\" : \"apache\" , \"simpleName\" : \"apache-7c99b8d54f-9txbf (default)\" , \"aggregationIdentifier\" : \"apache-7c99b8d54f\" , \"cattle\" : true , \"containerType\" : \"KubernetesPodContainer\" , \"identity\" : 2597272431 , \"experimentStartTime\" : \"2020-02-26T09:22:02.786886Z\" , \"knownMissingCapabilities\" : [] }, \"experimentType\" : \"STATE\" , \"selfHealingMethod\" : {}, \"startTime\" : \"2020-02-26T09:22:02.786886Z\" , \"lastSelfHealingTime\" : null , \"selfHealingCounter\" : 0 , \"experimentMethodName\" : \"starveRandomNumberGenerator.sh\" , \"experimentLayerName\" : \"KubernetesPodContainer\" , \"wasSelfHealingRequired\" : null } ]","title":"Random experiments"},{"location":"Labs/Lab 5/#lab-summary","text":"At the end of this exercise you should know: How to construct and run experiment suite","title":"Lab summary"},{"location":"Labs/Lab 6/","text":"Lab 6: Monitor Experiments In this lab we will demonstrate how you can monitor ongoing experiments. Splunk We can attach Splunk Monitor server to our Chaos Engine service and do a data post processing there. Start Splunk Go to you Chaos Engine instance and from the chaos-engine git repo run: shell command docker-compose -f docker-compose-splunk.yml up expected command WARNING : Found orphan containers ( chaos - engine_vault - loader_1 , chaos - engine_vault_1 , chaos - engine_chaosengine_1 ) for this project . If you removed or renamed this service in your compose file , you can run this command with the -- remove - orphans flag to clean it up . Starting chaos - engine_splunk_1 ... done Attaching to chaos - engine_splunk_1 splunk_1 | splunk_1 | Splunk > 4 TW splunk_1 | splunk_1 | Checking prerequisites ... splunk_1 | Checking http port [ 8000 ]: open splunk_1 | Checking mgmt port [ 8089 ]: open splunk_1 | Checking appserver port [ 127.0 . 0.1 : 8065 ]: open splunk_1 | Checking kvstore port [ 8191 ]: open splunk_1 | Checking configuration ... Done . splunk_1 | Checking critical directories ... Done splunk_1 | Checking indexes ... splunk_1 | Validated : _audit _internal _introspection _telemetry _thefishbucket history main summary splunk_1 | Done splunk_1 | Checking filesystem compatibility ... Done splunk_1 | Checking conf files for problems ... splunk_1 | Done splunk_1 | Checking default conf files for edits ... splunk_1 | Validating installed files against hashes from '/opt/splunk/splunk-7.0.3-fa31da744b51-linux-2.6-x86_64-manifest' splunk_1 | File '/opt/splunk/etc/apps/splunk_httpinput/default/inputs.conf' changed . splunk_1 | Problems were found , please review your files and move customizations to local splunk_1 | All preliminary checks passed . splunk_1 | splunk_1 | Starting splunk server daemon ( splunkd )... splunk_1 | Done splunk_1 | splunk_1 | splunk_1 | Waiting for web server at http :// 127.0 . 0.1 : 8000 to be available ..... Done splunk_1 | splunk_1 | splunk_1 | If you get stuck , we ' re here to help . splunk_1 | Look for answers here : http :// docs . splunk . com splunk_1 | splunk_1 | The Splunk web interface is at http :// b987dc22238f : 8000 splunk_1 | Chaos Engine Dashboard Splunk instance will be preloaded with a dashboard for Chaos Experiments monitoring. https:// ${ CHAOS_ENGINE_IP } :9000/en-US/app/search/chaos_overview Notification Modules The Chaos Engine can send notifications to multiple channels. At the moment it supports: Slack DataDog Event Stream XMPP / Jabber For detail description of each notification channel and message content see documentation. XMPP Module Let's activate XMPP module to better understand how it works. Deploy test XMPP server We need to deploy a XMPP server who will be processing our notifications. Start XMPP server Go to your Chaos Engine instance and run ejabberd server: shell command docker run -it --rm --name ejabberd \\ -p 5222 :5222 \\ -p 5269 :5269 \\ -p 5443 :5443 \\ -p 1883 :1883 \\ -p 5280 :5280 ejabberd/ecs expected output 2020 - 02 - 27 12 : 21 : 52.274868 + 00 : 00 [ critical ] Failed to set logging : { error , { handler_not_added , { invalid_config , logger_std_h , #{file => \"/home/ejabberd/logs/ejabberd.log\" }}}} 2020 - 02 - 27 12 : 21 : 52.654284 + 00 : 00 [ info ] Loading configuration from / home / ejabberd / conf / ejabberd . yml 2020 - 02 - 27 12 : 21 : 52.703112 + 00 : 00 [ warning ] ACME directory URL https : //acme-v01.api.letsencrypt.org defined in option acme->ca_url is deprecated and was automatically replaced with https://acme-v02.api.letsencrypt.org/directory. Please adjust your configuration file accordingly. Hint: run `ejabberdctl dump-config` command to view current configuration as it is seen by ejabberd. 2020 - 02 - 27 12 : 21 : 52.703553 + 00 : 00 [ warning ] Option ' log_rotate_date ' is deprecated and has no effect anymore . Please remove it from the configuration . 2020 - 02 - 27 12 : 21 : 52.703873 + 00 : 00 [ warning ] Option ' log_rate_limit ' is deprecated and has no effect anymore . Please remove it from the configuration . 2020 - 02 - 27 12 : 21 : 53.466033 + 00 : 00 [ info ] Configuration loaded successfully 2020 - 02 - 27 12 : 21 : 53.766025 + 00 : 00 [ info ] Building language translation cache 2020 - 02 - 27 12 : 21 : 54.377753 + 00 : 00 [ info ] Creating Mnesia ram table ' ejabberd_commands ' 2020 - 02 - 27 12 : 21 : 54.477696 + 00 : 00 [ info ] Creating Mnesia ram table ' route ' 2020 - 02 - 27 12 : 21 : 54.490312 + 00 : 00 [ info ] Creating Mnesia ram table ' route_multicast ' 2020 - 02 - 27 12 : 21 : 54.515036 + 00 : 00 [ info ] Creating Mnesia ram table ' session ' 2020 - 02 - 27 12 : 21 : 54.524188 + 00 : 00 [ info ] Creating Mnesia ram table ' session_counter ' 2020 - 02 - 27 12 : 21 : 54.541913 + 00 : 00 [ info ] Creating Mnesia ram table ' s2s ' 2020 - 02 - 27 12 : 21 : 54.550388 + 00 : 00 [ info ] Creating Mnesia ram table ' temporarily_blocked ' 2020 - 02 - 27 12 : 21 : 54.568955 + 00 : 00 [ info ] Loading modules for localhost 2020 - 02 - 27 12 : 21 : 54.570129 + 00 : 00 [ info ] Creating Mnesia ram table ' mod_register_ip ' 2020 - 02 - 27 12 : 21 : 54.577959 + 00 : 00 [ info ] Creating Mnesia disc table ' sr_group ' 2020 - 02 - 27 12 : 21 : 54.585388 + 00 : 00 [ info ] Creating Mnesia disc table ' sr_user ' 2020 - 02 - 27 12 : 21 : 54.596781 + 00 : 00 [ info ] Creating Mnesia disc_only table ' privacy ' 2020 - 02 - 27 12 : 21 : 54.628718 + 00 : 00 [ warning ] Mnesia backend for mod_mam is not recommended : it ' s limited to 2 GB and often gets corrupted when reaching this limit . SQL backend is recommended . Namely , for small servers SQLite is a preferred choice because it ' s very easy to configure . 2020 - 02 - 27 12 : 21 : 54.629141 + 00 : 00 [ info ] Creating Mnesia disc_only table ' archive_msg ' 2020 - 02 - 27 12 : 21 : 54.640068 + 00 : 00 [ info ] Creating Mnesia disc_only table ' archive_prefs ' 2020 - 02 - 27 12 : 21 : 54.685725 + 00 : 00 [ info ] Creating Mnesia disc table ' muc_room ' 2020 - 02 - 27 12 : 21 : 54.694844 + 00 : 00 [ info ] Creating Mnesia disc table ' muc_registered ' 2020 - 02 - 27 12 : 21 : 54.704680 + 00 : 00 [ info ] Creating Mnesia ram table ' muc_online_room ' 2020 - 02 - 27 12 : 21 : 54.717231 + 00 : 00 [ info ] Creating Mnesia disc_only table ' vcard ' 2020 - 02 - 27 12 : 21 : 54.726915 + 00 : 00 [ info ] Creating Mnesia disc table ' vcard_search ' 2020 - 02 - 27 12 : 21 : 54.750776 + 00 : 00 [ info ] Creating Mnesia disc_only table ' motd ' 2020 - 02 - 27 12 : 21 : 54.760577 + 00 : 00 [ info ] Creating Mnesia disc_only table ' motd_users ' 2020 - 02 - 27 12 : 21 : 54.789235 + 00 : 00 [ info ] Creating Mnesia ram table ' bosh ' 2020 - 02 - 27 12 : 21 : 54.797795 + 00 : 00 [ info ] Creating Mnesia disc_only table ' push_session ' 2020 - 02 - 27 12 : 21 : 54.819676 + 00 : 00 [ info ] Creating Mnesia disc_only table ' roster ' 2020 - 02 - 27 12 : 21 : 54.845965 + 00 : 00 [ info ] Creating Mnesia disc_only table ' roster_version ' 2020 - 02 - 27 12 : 21 : 54.922432 + 00 : 00 [ info ] Creating Mnesia disc_only table ' last_activity ' 2020 - 02 - 27 12 : 21 : 54.947673 + 00 : 00 [ info ] Creating Mnesia disc_only table ' offline_msg ' 2020 - 02 - 27 12 : 21 : 54.998384 + 00 : 00 [ info ] Creating Mnesia ram table ' sip_session ' 2020 - 02 - 27 12 : 21 : 55.038610 + 00 : 00 [ info ] Creating Mnesia disc_only table ' caps_features ' 2020 - 02 - 27 12 : 21 : 55.048919 + 00 : 00 [ info ] Creating Mnesia ram table ' pubsub_last_item ' 2020 - 02 - 27 12 : 21 : 55.059239 + 00 : 00 [ info ] Creating Mnesia disc table ' pubsub_index ' 2020 - 02 - 27 12 : 21 : 55.073598 + 00 : 00 [ info ] Creating Mnesia disc table ' pubsub_node ' 2020 - 02 - 27 12 : 21 : 55.084673 + 00 : 00 [ info ] Creating Mnesia disc table ' pubsub_state ' 2020 - 02 - 27 12 : 21 : 55.093349 + 00 : 00 [ info ] Creating Mnesia disc_only table ' pubsub_item ' 2020 - 02 - 27 12 : 21 : 55.108872 + 00 : 00 [ info ] Creating Mnesia disc table ' pubsub_orphan ' 2020 - 02 - 27 12 : 21 : 55.117947 + 00 : 00 [ info ] Creating Mnesia disc_only table ' private_storage ' 2020 - 02 - 27 12 : 21 : 55.145023 + 00 : 00 [ info ] Creating Mnesia disc_only table ' mqtt_pub ' 2020 - 02 - 27 12 : 21 : 55.160187 + 00 : 00 [ info ] Creating Mnesia ram table ' mqtt_session ' 2020 - 02 - 27 12 : 21 : 55.169008 + 00 : 00 [ info ] Creating Mnesia ram table ' mqtt_sub ' 2020 - 02 - 27 12 : 21 : 55.193342 + 00 : 00 [ info ] Building MQTT cache for localhost , this may take a while 2020 - 02 - 27 12 : 21 : 55.212517 + 00 : 00 [ info ] Creating Mnesia ram table ' bytestream ' 2020 - 02 - 27 12 : 21 : 55.228890 + 00 : 00 [ info ] Creating Mnesia disc_only table ' passwd ' 2020 - 02 - 27 12 : 21 : 55.238541 + 00 : 00 [ info ] Creating Mnesia ram table ' reg_users_counter ' 2020 - 02 - 27 12 : 21 : 55.261166 + 00 : 00 [ info ] Creating Mnesia disc_only table ' oauth_token ' 2020 - 02 - 27 12 : 21 : 55.272232 + 00 : 00 [ info ] Creating Mnesia disc table ' oauth_client ' 2020 - 02 - 27 12 : 21 : 55.328426 + 00 : 00 [ info ] Waiting for Mnesia synchronization to complete 2020 - 02 - 27 12 : 21 : 55.401107 + 00 : 00 [ warning ] Invalid certificate in / home / ejabberd / conf / server . pem : at line 53 : self - signed certificate 2020 - 02 - 27 12 : 21 : 55.620877 + 00 : 00 [ warning ] No certificate found matching conference . localhost 2020 - 02 - 27 12 : 21 : 55.621084 + 00 : 00 [ warning ] No certificate found matching upload . localhost 2020 - 02 - 27 12 : 21 : 55.621280 + 00 : 00 [ warning ] No certificate found matching proxy . localhost 2020 - 02 - 27 12 : 21 : 55.621436 + 00 : 00 [ warning ] No certificate found matching pubsub . localhost 2020 - 02 - 27 12 : 21 : 55.621592 + 00 : 00 [ info ] ejabberd 20.1.0 is started in the node ejabberd @97 ac81e5b1ab in 3.36 s 2020 - 02 - 27 12 : 21 : 55.624763 + 00 : 00 [ info ] Start accepting TCP connections at [ :: ] : 5222 for ejabberd_c2s 2020 - 02 - 27 12 : 21 : 55.624997 + 00 : 00 [ info ] Start accepting TCP connections at [ :: ] : 5269 for ejabberd_s2s_in 2020 - 02 - 27 12 : 21 : 55.626317 + 00 : 00 [ info ] Start accepting TLS connections at [ :: ] : 5443 for ejabberd_http 2020 - 02 - 27 12 : 21 : 55.626556 + 00 : 00 [ info ] Start accepting TCP connections at [ :: ] : 5280 for ejabberd_http 2020 - 02 - 27 12 : 21 : 55.628548 + 00 : 00 [ info ] Start accepting TCP connections at [ :: ] : 1883 for mod_mqtt 2020 - 02 - 27 12 : 21 : 55.630578 + 00 : 00 [ info ] Start accepting TCP connections at 172.17.0.2 : 7777 for mod_proxy65_stream Adjust configuration Add ${CHAOS_ENGINE_IP} into the list of hosts in ejabberd config: docker exec -it ejabberd vi /home/ejabberd/conf/ejabberd.yml Reload server configuration docker exec -it ejabberd bin/ejabberdctl reload_config Provision test users In the next step we are going to provision two users chaos and test . chaos is an account to be used by the Chaos Engine. test is a user who will receive notifications. docker exec -it ejabberd bin/ejabberdctl register test localhost test docker exec -it ejabberd bin/ejabberdctl register chaos localhost test docker exec -it ejabberd bin/ejabberdctl register test ${ CHAOS_ENGINE_IP } test docker exec -it ejabberd bin/ejabberdctl register chaos ${ CHAOS_ENGINE_IP } test Connect a jabber client Recommended client is Pidgin with XMPP plugin. If you don't have Pidgin installed or your organization blocks XMPP traffic you can used finch ( finch cheat cheat ) that is installed on your chaos engine machine. Start your preferred client and and new account with following parameters: user: test password: test domain: localhost server: ${CHAOS_ENGINE_IP} Update Chaos Engine configuration Stop your Chaos Engine instance by running: docker-compose stop Update ./developer-tools/vault-loader/vault-secrets.json file in the Chaos Engine git repo with following parameters: \"xmpp.enabled\" : \"true\" , \"xmpp.user\" : \"chaos\" , \"xmpp.password\" : \"test\" , \"xmpp.domain\" : \"localhost\" , \"xmpp.hostname\" : \"${CHAOS_ENGINE_IP}\" , \"xmpp.serverCertFingerprint\" : \"CERTSHA256:f9:16:59:0b:93:72:66:a4:9a:db:df:2a:7f:8b:a3:cf:44:2b:a2:31:a8:1a:72:f5:7d:43:76:21:c6:2c:b3:81\" , \"xmpp.recipients\" : \"test@localhost\" Start the Chaos Engine again docker-compose stop If everything was configured well you should see new message popping up in your IM client. Trigger experiment api and check content of incoming messages. curl -X POST \"http:// ${ CHAOS_ENINE_IP } :8080/experiment/start\"","title":"Lab 6: Monitor Experiments"},{"location":"Labs/Lab 6/#lab-6-monitor-experiments","text":"In this lab we will demonstrate how you can monitor ongoing experiments.","title":"Lab 6: Monitor Experiments"},{"location":"Labs/Lab 6/#splunk","text":"We can attach Splunk Monitor server to our Chaos Engine service and do a data post processing there.","title":"Splunk"},{"location":"Labs/Lab 6/#start-splunk","text":"Go to you Chaos Engine instance and from the chaos-engine git repo run: shell command docker-compose -f docker-compose-splunk.yml up expected command WARNING : Found orphan containers ( chaos - engine_vault - loader_1 , chaos - engine_vault_1 , chaos - engine_chaosengine_1 ) for this project . If you removed or renamed this service in your compose file , you can run this command with the -- remove - orphans flag to clean it up . Starting chaos - engine_splunk_1 ... done Attaching to chaos - engine_splunk_1 splunk_1 | splunk_1 | Splunk > 4 TW splunk_1 | splunk_1 | Checking prerequisites ... splunk_1 | Checking http port [ 8000 ]: open splunk_1 | Checking mgmt port [ 8089 ]: open splunk_1 | Checking appserver port [ 127.0 . 0.1 : 8065 ]: open splunk_1 | Checking kvstore port [ 8191 ]: open splunk_1 | Checking configuration ... Done . splunk_1 | Checking critical directories ... Done splunk_1 | Checking indexes ... splunk_1 | Validated : _audit _internal _introspection _telemetry _thefishbucket history main summary splunk_1 | Done splunk_1 | Checking filesystem compatibility ... Done splunk_1 | Checking conf files for problems ... splunk_1 | Done splunk_1 | Checking default conf files for edits ... splunk_1 | Validating installed files against hashes from '/opt/splunk/splunk-7.0.3-fa31da744b51-linux-2.6-x86_64-manifest' splunk_1 | File '/opt/splunk/etc/apps/splunk_httpinput/default/inputs.conf' changed . splunk_1 | Problems were found , please review your files and move customizations to local splunk_1 | All preliminary checks passed . splunk_1 | splunk_1 | Starting splunk server daemon ( splunkd )... splunk_1 | Done splunk_1 | splunk_1 | splunk_1 | Waiting for web server at http :// 127.0 . 0.1 : 8000 to be available ..... Done splunk_1 | splunk_1 | splunk_1 | If you get stuck , we ' re here to help . splunk_1 | Look for answers here : http :// docs . splunk . com splunk_1 | splunk_1 | The Splunk web interface is at http :// b987dc22238f : 8000 splunk_1 |","title":"Start Splunk"},{"location":"Labs/Lab 6/#chaos-engine-dashboard","text":"Splunk instance will be preloaded with a dashboard for Chaos Experiments monitoring. https:// ${ CHAOS_ENGINE_IP } :9000/en-US/app/search/chaos_overview","title":"Chaos Engine Dashboard"},{"location":"Labs/Lab 6/#notification-modules","text":"The Chaos Engine can send notifications to multiple channels. At the moment it supports: Slack DataDog Event Stream XMPP / Jabber For detail description of each notification channel and message content see documentation.","title":"Notification Modules"},{"location":"Labs/Lab 6/#xmpp-module","text":"Let's activate XMPP module to better understand how it works.","title":"XMPP Module"},{"location":"Labs/Lab 6/#deploy-test-xmpp-server","text":"We need to deploy a XMPP server who will be processing our notifications.","title":"Deploy test XMPP server"},{"location":"Labs/Lab 6/#start-xmpp-server","text":"Go to your Chaos Engine instance and run ejabberd server: shell command docker run -it --rm --name ejabberd \\ -p 5222 :5222 \\ -p 5269 :5269 \\ -p 5443 :5443 \\ -p 1883 :1883 \\ -p 5280 :5280 ejabberd/ecs expected output 2020 - 02 - 27 12 : 21 : 52.274868 + 00 : 00 [ critical ] Failed to set logging : { error , { handler_not_added , { invalid_config , logger_std_h , #{file => \"/home/ejabberd/logs/ejabberd.log\" }}}} 2020 - 02 - 27 12 : 21 : 52.654284 + 00 : 00 [ info ] Loading configuration from / home / ejabberd / conf / ejabberd . yml 2020 - 02 - 27 12 : 21 : 52.703112 + 00 : 00 [ warning ] ACME directory URL https : //acme-v01.api.letsencrypt.org defined in option acme->ca_url is deprecated and was automatically replaced with https://acme-v02.api.letsencrypt.org/directory. Please adjust your configuration file accordingly. Hint: run `ejabberdctl dump-config` command to view current configuration as it is seen by ejabberd. 2020 - 02 - 27 12 : 21 : 52.703553 + 00 : 00 [ warning ] Option ' log_rotate_date ' is deprecated and has no effect anymore . Please remove it from the configuration . 2020 - 02 - 27 12 : 21 : 52.703873 + 00 : 00 [ warning ] Option ' log_rate_limit ' is deprecated and has no effect anymore . Please remove it from the configuration . 2020 - 02 - 27 12 : 21 : 53.466033 + 00 : 00 [ info ] Configuration loaded successfully 2020 - 02 - 27 12 : 21 : 53.766025 + 00 : 00 [ info ] Building language translation cache 2020 - 02 - 27 12 : 21 : 54.377753 + 00 : 00 [ info ] Creating Mnesia ram table ' ejabberd_commands ' 2020 - 02 - 27 12 : 21 : 54.477696 + 00 : 00 [ info ] Creating Mnesia ram table ' route ' 2020 - 02 - 27 12 : 21 : 54.490312 + 00 : 00 [ info ] Creating Mnesia ram table ' route_multicast ' 2020 - 02 - 27 12 : 21 : 54.515036 + 00 : 00 [ info ] Creating Mnesia ram table ' session ' 2020 - 02 - 27 12 : 21 : 54.524188 + 00 : 00 [ info ] Creating Mnesia ram table ' session_counter ' 2020 - 02 - 27 12 : 21 : 54.541913 + 00 : 00 [ info ] Creating Mnesia ram table ' s2s ' 2020 - 02 - 27 12 : 21 : 54.550388 + 00 : 00 [ info ] Creating Mnesia ram table ' temporarily_blocked ' 2020 - 02 - 27 12 : 21 : 54.568955 + 00 : 00 [ info ] Loading modules for localhost 2020 - 02 - 27 12 : 21 : 54.570129 + 00 : 00 [ info ] Creating Mnesia ram table ' mod_register_ip ' 2020 - 02 - 27 12 : 21 : 54.577959 + 00 : 00 [ info ] Creating Mnesia disc table ' sr_group ' 2020 - 02 - 27 12 : 21 : 54.585388 + 00 : 00 [ info ] Creating Mnesia disc table ' sr_user ' 2020 - 02 - 27 12 : 21 : 54.596781 + 00 : 00 [ info ] Creating Mnesia disc_only table ' privacy ' 2020 - 02 - 27 12 : 21 : 54.628718 + 00 : 00 [ warning ] Mnesia backend for mod_mam is not recommended : it ' s limited to 2 GB and often gets corrupted when reaching this limit . SQL backend is recommended . Namely , for small servers SQLite is a preferred choice because it ' s very easy to configure . 2020 - 02 - 27 12 : 21 : 54.629141 + 00 : 00 [ info ] Creating Mnesia disc_only table ' archive_msg ' 2020 - 02 - 27 12 : 21 : 54.640068 + 00 : 00 [ info ] Creating Mnesia disc_only table ' archive_prefs ' 2020 - 02 - 27 12 : 21 : 54.685725 + 00 : 00 [ info ] Creating Mnesia disc table ' muc_room ' 2020 - 02 - 27 12 : 21 : 54.694844 + 00 : 00 [ info ] Creating Mnesia disc table ' muc_registered ' 2020 - 02 - 27 12 : 21 : 54.704680 + 00 : 00 [ info ] Creating Mnesia ram table ' muc_online_room ' 2020 - 02 - 27 12 : 21 : 54.717231 + 00 : 00 [ info ] Creating Mnesia disc_only table ' vcard ' 2020 - 02 - 27 12 : 21 : 54.726915 + 00 : 00 [ info ] Creating Mnesia disc table ' vcard_search ' 2020 - 02 - 27 12 : 21 : 54.750776 + 00 : 00 [ info ] Creating Mnesia disc_only table ' motd ' 2020 - 02 - 27 12 : 21 : 54.760577 + 00 : 00 [ info ] Creating Mnesia disc_only table ' motd_users ' 2020 - 02 - 27 12 : 21 : 54.789235 + 00 : 00 [ info ] Creating Mnesia ram table ' bosh ' 2020 - 02 - 27 12 : 21 : 54.797795 + 00 : 00 [ info ] Creating Mnesia disc_only table ' push_session ' 2020 - 02 - 27 12 : 21 : 54.819676 + 00 : 00 [ info ] Creating Mnesia disc_only table ' roster ' 2020 - 02 - 27 12 : 21 : 54.845965 + 00 : 00 [ info ] Creating Mnesia disc_only table ' roster_version ' 2020 - 02 - 27 12 : 21 : 54.922432 + 00 : 00 [ info ] Creating Mnesia disc_only table ' last_activity ' 2020 - 02 - 27 12 : 21 : 54.947673 + 00 : 00 [ info ] Creating Mnesia disc_only table ' offline_msg ' 2020 - 02 - 27 12 : 21 : 54.998384 + 00 : 00 [ info ] Creating Mnesia ram table ' sip_session ' 2020 - 02 - 27 12 : 21 : 55.038610 + 00 : 00 [ info ] Creating Mnesia disc_only table ' caps_features ' 2020 - 02 - 27 12 : 21 : 55.048919 + 00 : 00 [ info ] Creating Mnesia ram table ' pubsub_last_item ' 2020 - 02 - 27 12 : 21 : 55.059239 + 00 : 00 [ info ] Creating Mnesia disc table ' pubsub_index ' 2020 - 02 - 27 12 : 21 : 55.073598 + 00 : 00 [ info ] Creating Mnesia disc table ' pubsub_node ' 2020 - 02 - 27 12 : 21 : 55.084673 + 00 : 00 [ info ] Creating Mnesia disc table ' pubsub_state ' 2020 - 02 - 27 12 : 21 : 55.093349 + 00 : 00 [ info ] Creating Mnesia disc_only table ' pubsub_item ' 2020 - 02 - 27 12 : 21 : 55.108872 + 00 : 00 [ info ] Creating Mnesia disc table ' pubsub_orphan ' 2020 - 02 - 27 12 : 21 : 55.117947 + 00 : 00 [ info ] Creating Mnesia disc_only table ' private_storage ' 2020 - 02 - 27 12 : 21 : 55.145023 + 00 : 00 [ info ] Creating Mnesia disc_only table ' mqtt_pub ' 2020 - 02 - 27 12 : 21 : 55.160187 + 00 : 00 [ info ] Creating Mnesia ram table ' mqtt_session ' 2020 - 02 - 27 12 : 21 : 55.169008 + 00 : 00 [ info ] Creating Mnesia ram table ' mqtt_sub ' 2020 - 02 - 27 12 : 21 : 55.193342 + 00 : 00 [ info ] Building MQTT cache for localhost , this may take a while 2020 - 02 - 27 12 : 21 : 55.212517 + 00 : 00 [ info ] Creating Mnesia ram table ' bytestream ' 2020 - 02 - 27 12 : 21 : 55.228890 + 00 : 00 [ info ] Creating Mnesia disc_only table ' passwd ' 2020 - 02 - 27 12 : 21 : 55.238541 + 00 : 00 [ info ] Creating Mnesia ram table ' reg_users_counter ' 2020 - 02 - 27 12 : 21 : 55.261166 + 00 : 00 [ info ] Creating Mnesia disc_only table ' oauth_token ' 2020 - 02 - 27 12 : 21 : 55.272232 + 00 : 00 [ info ] Creating Mnesia disc table ' oauth_client ' 2020 - 02 - 27 12 : 21 : 55.328426 + 00 : 00 [ info ] Waiting for Mnesia synchronization to complete 2020 - 02 - 27 12 : 21 : 55.401107 + 00 : 00 [ warning ] Invalid certificate in / home / ejabberd / conf / server . pem : at line 53 : self - signed certificate 2020 - 02 - 27 12 : 21 : 55.620877 + 00 : 00 [ warning ] No certificate found matching conference . localhost 2020 - 02 - 27 12 : 21 : 55.621084 + 00 : 00 [ warning ] No certificate found matching upload . localhost 2020 - 02 - 27 12 : 21 : 55.621280 + 00 : 00 [ warning ] No certificate found matching proxy . localhost 2020 - 02 - 27 12 : 21 : 55.621436 + 00 : 00 [ warning ] No certificate found matching pubsub . localhost 2020 - 02 - 27 12 : 21 : 55.621592 + 00 : 00 [ info ] ejabberd 20.1.0 is started in the node ejabberd @97 ac81e5b1ab in 3.36 s 2020 - 02 - 27 12 : 21 : 55.624763 + 00 : 00 [ info ] Start accepting TCP connections at [ :: ] : 5222 for ejabberd_c2s 2020 - 02 - 27 12 : 21 : 55.624997 + 00 : 00 [ info ] Start accepting TCP connections at [ :: ] : 5269 for ejabberd_s2s_in 2020 - 02 - 27 12 : 21 : 55.626317 + 00 : 00 [ info ] Start accepting TLS connections at [ :: ] : 5443 for ejabberd_http 2020 - 02 - 27 12 : 21 : 55.626556 + 00 : 00 [ info ] Start accepting TCP connections at [ :: ] : 5280 for ejabberd_http 2020 - 02 - 27 12 : 21 : 55.628548 + 00 : 00 [ info ] Start accepting TCP connections at [ :: ] : 1883 for mod_mqtt 2020 - 02 - 27 12 : 21 : 55.630578 + 00 : 00 [ info ] Start accepting TCP connections at 172.17.0.2 : 7777 for mod_proxy65_stream","title":"Start XMPP server"},{"location":"Labs/Lab 6/#adjust-configuration","text":"Add ${CHAOS_ENGINE_IP} into the list of hosts in ejabberd config: docker exec -it ejabberd vi /home/ejabberd/conf/ejabberd.yml Reload server configuration docker exec -it ejabberd bin/ejabberdctl reload_config","title":"Adjust configuration"},{"location":"Labs/Lab 6/#provision-test-users","text":"In the next step we are going to provision two users chaos and test . chaos is an account to be used by the Chaos Engine. test is a user who will receive notifications. docker exec -it ejabberd bin/ejabberdctl register test localhost test docker exec -it ejabberd bin/ejabberdctl register chaos localhost test docker exec -it ejabberd bin/ejabberdctl register test ${ CHAOS_ENGINE_IP } test docker exec -it ejabberd bin/ejabberdctl register chaos ${ CHAOS_ENGINE_IP } test","title":"Provision test users"},{"location":"Labs/Lab 6/#connect-a-jabber-client","text":"Recommended client is Pidgin with XMPP plugin. If you don't have Pidgin installed or your organization blocks XMPP traffic you can used finch ( finch cheat cheat ) that is installed on your chaos engine machine. Start your preferred client and and new account with following parameters: user: test password: test domain: localhost server: ${CHAOS_ENGINE_IP}","title":"Connect a jabber client"},{"location":"Labs/Lab 6/#update-chaos-engine-configuration","text":"Stop your Chaos Engine instance by running: docker-compose stop Update ./developer-tools/vault-loader/vault-secrets.json file in the Chaos Engine git repo with following parameters: \"xmpp.enabled\" : \"true\" , \"xmpp.user\" : \"chaos\" , \"xmpp.password\" : \"test\" , \"xmpp.domain\" : \"localhost\" , \"xmpp.hostname\" : \"${CHAOS_ENGINE_IP}\" , \"xmpp.serverCertFingerprint\" : \"CERTSHA256:f9:16:59:0b:93:72:66:a4:9a:db:df:2a:7f:8b:a3:cf:44:2b:a2:31:a8:1a:72:f5:7d:43:76:21:c6:2c:b3:81\" , \"xmpp.recipients\" : \"test@localhost\" Start the Chaos Engine again docker-compose stop If everything was configured well you should see new message popping up in your IM client. Trigger experiment api and check content of incoming messages. curl -X POST \"http:// ${ CHAOS_ENINE_IP } :8080/experiment/start\"","title":"Update Chaos Engine configuration"},{"location":"Labs/Lab 7/","text":"Lab 7: Clean up Make sure you freed all the resources you have created during this workshop. Delete Chaos Engine machine gcloud compute instances delete --zone \"europe-west2-c\" \"chaos-engine\" Purge firewall rules gcloud compute firewall-rules delete chaos-engine-inbound Delete Kubernetes cluster gcloud container clusters delete --zone \"europe-west1-b\" \"chaos-engine-victim\"","title":"Lab 7: Clean up"},{"location":"Labs/Lab 7/#lab-7-clean-up","text":"Make sure you freed all the resources you have created during this workshop.","title":"Lab 7: Clean up"},{"location":"Labs/Lab 7/#delete-chaos-engine-machine","text":"gcloud compute instances delete --zone \"europe-west2-c\" \"chaos-engine\"","title":"Delete Chaos Engine machine"},{"location":"Labs/Lab 7/#purge-firewall-rules","text":"gcloud compute firewall-rules delete chaos-engine-inbound","title":"Purge firewall rules"},{"location":"Labs/Lab 7/#delete-kubernetes-cluster","text":"gcloud container clusters delete --zone \"europe-west1-b\" \"chaos-engine-victim\"","title":"Delete Kubernetes cluster"}]}